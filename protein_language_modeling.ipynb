{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "589e2dce",
      "metadata": {},
      "source": [
        "### Installation of dependencies on a vast or runpod machine with latest pytorch docker image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c5bf8d4",
      "metadata": {
        "id": "4c5bf8d4"
      },
      "outputs": [],
      "source": [
        "! pip install transformers[torch] evaluate datasets requests pandas scikit-learn peft bitsandbytes matplotlib sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19e8f77c",
      "metadata": {
        "id": "19e8f77c"
      },
      "outputs": [],
      "source": [
        "# !apt install git-lfs\n",
        "# pip install wandb\n",
        "# wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c0749e1",
      "metadata": {
        "id": "5c0749e1"
      },
      "source": [
        "# Fine-Tuning Protein Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d81db83",
      "metadata": {
        "id": "1d81db83"
      },
      "source": [
        "Inspired by [a blog post](https://huggingface.co/blog/deep-learning-with-proteins), in this notebook, we're going to do some transfer learning to fine-tune some large, pre-trained protein language models on the classification task of cath architecture classes. \n",
        "\n",
        "The specific models we're going to use here are either ESM-2 or prot_bert, which are the state-of-the-art protein language models.\n",
        "\n",
        "There are several ESM-2 checkpoints with differing model sizes. Larger models will generally have better accuracy, but they require more GPU memory and will take much longer to train. The available ESM-2 checkpoints are:\n",
        "\n",
        "| Checkpoint name | Num layers | Num parameters |\n",
        "|------------------------------|----|----------|\n",
        "| `esm2_t48_15B_UR50D`         | 48 | 15B     |\n",
        "| `esm2_t36_3B_UR50D`          | 36 | 3B      |\n",
        "| `esm2_t33_650M_UR50D`        | 33 | 650M    |\n",
        "| `esm2_t30_150M_UR50D`        | 30 | 150M    |\n",
        "| `esm2_t12_35M_UR50D`         | 12 | 35M     |\n",
        "| `esm2_t6_8M_UR50D`           | 6  | 8M      |\n",
        "\n",
        "We will use the `esm2_t12_35M_UR50D`  and `esm2_t33_650M_UR50D` checkpoints for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32e605a2",
      "metadata": {
        "id": "32e605a2"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"facebook/esm2_t33_650M_UR50D\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8e6ac19",
      "metadata": {
        "id": "a8e6ac19"
      },
      "source": [
        "# Sequence classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3eb400c",
      "metadata": {
        "id": "c3eb400c"
      },
      "source": [
        "Given that we have the protein sequences available in our dataset, we can perform supervised learning of the CATH labels given the sequences as inputs. More specifically, we will do finetuning of the protein LLMs on the sequence-cath_label pairs to learn the classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5bc122f",
      "metadata": {
        "id": "c5bc122f"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c91d394",
      "metadata": {
        "id": "4c91d394"
      },
      "source": [
        "Our goal is to create a pair of lists: `sequences` and `labels`. `sequences` will be a list of protein sequences, which will just be strings like \"MNKL...\", where each letter represents a single amino acid in the complete protein. `labels` will be a list of the category for each sequence. The categories will just be integers from 0 to 9. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7560840d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Change to the desired directory\n",
        "# os.chdir(\"/root\")\n",
        "# Verify the change\n",
        "print(os.listdir(\"./\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "561ed6f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(\"./data/train.csv\")\n",
        "val_df = pd.read_csv(\"./data/val.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ee257c",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_sequences = train_df[\"sequences\"].tolist()\n",
        "test_sequences = val_df[\"sequences\"].tolist()\n",
        "train_labels = train_df[\"label\"].tolist()\n",
        "test_labels = val_df[\"label\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cfd1e1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.Series(train_labels).value_counts(sort=True, ascending=False).plot(\n",
        "    kind=\"bar\", backend=\"matplotlib\", figsize=(10, 5)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cccc715",
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.Series(test_labels).value_counts(sort=True, ascending=False).plot(\n",
        "    kind=\"bar\", backend=\"matplotlib\", figsize=(10, 5)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d29b4ed",
      "metadata": {
        "id": "7d29b4ed"
      },
      "source": [
        "## Tokenizing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c02baaf7",
      "metadata": {
        "id": "c02baaf7"
      },
      "source": [
        "All inputs to neural nets must be numerical. The process of converting strings into numerical indices suitable for a neural net is called **tokenization**. For natural language this can be quite complex, as usually the network's vocabulary will not contain every possible word, which means the tokenizer must handle splitting rarer words into pieces, as well as all the complexities of capitalization and unicode characters and so on.\n",
        "\n",
        "With proteins, however, things are very easy. In protein language models, each amino acid is converted to a single token. Every model on `transformers` comes with an associated `tokenizer` that handles tokenization for it, and protein language models are no different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddbe2b2d",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "cc61f599adc641da8d40eefac0179aa3",
            "4e3d35bc47c74852a5256f5b34312030",
            "8b7f3b4a47d8437f9e9cc6648cfc8984"
          ]
        },
        "id": "ddbe2b2d",
        "outputId": "131761f1-5609-4fc4-f4cc-5ed87c662774"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a719808",
      "metadata": {
        "id": "9a719808"
      },
      "source": [
        "This looks good! We can see that our sequence has been converted into `input_ids`, which is the tokenized sequence, and an `attention_mask`. The attention mask handles the case when we have sequences of variable length - in those cases, the shorter sequences are padded with blank \"padding\" tokens, and the attention mask is padded with 0s to indicate that those tokens should be ignored by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56e26ddf",
      "metadata": {
        "id": "56e26ddf"
      },
      "outputs": [],
      "source": [
        "train_tokenized = tokenizer(train_sequences)\n",
        "test_tokenized = tokenizer(test_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df3681d1",
      "metadata": {
        "id": "df3681d1"
      },
      "source": [
        "## Dataset creation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85089e49",
      "metadata": {
        "id": "85089e49"
      },
      "source": [
        "Now we want to turn this data into a dataset that PyTorch can load samples from. We can use the HuggingFace `Dataset` class for this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb79ba6c",
      "metadata": {
        "id": "fb79ba6c",
        "outputId": "a2eacf43-ed76-48f1-c80a-8ca3955c471f"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = Dataset.from_dict(train_tokenized)\n",
        "test_dataset = Dataset.from_dict(test_tokenized)\n",
        "\n",
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "090acc0d",
      "metadata": {
        "id": "090acc0d",
        "outputId": "c34d29c5-a5e7-455f-c80d-d66db88445af"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
        "test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
        "train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ced9aaa8",
      "metadata": {
        "id": "ced9aaa8"
      },
      "source": [
        "Looks good! We're ready for training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af074a5c",
      "metadata": {
        "id": "af074a5c"
      },
      "source": [
        "## Model loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc164b49",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "0fac5f3b8b894d21aee5d90b61e93313",
            "4a5dd63ce59940babe11af50992fcd2a"
          ]
        },
        "id": "fc164b49",
        "outputId": "d842926f-3df9-40a6-b1a3-889c961b5c78"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "\n",
        "num_labels = max(train_labels + test_labels) + 1  # Add 1 since 0 can be a label\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels,\n",
        ")\n",
        "\n",
        "######## for lora experiments   ############\n",
        "\n",
        "# from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# def find_target_modules(model):\n",
        "#     target_modules = []\n",
        "#     for name, module in model.named_modules():\n",
        "#         if isinstance(module, torch.nn.Linear):\n",
        "#             target_modules.append(name)\n",
        "#     return target_modules\n",
        "\n",
        "\n",
        "# # Get the target modules\n",
        "# target_modules = find_target_modules(model)\n",
        "# peft_config = LoraConfig(\n",
        "#     task_type=TaskType.SEQ_CLS,\n",
        "#     inference_mode=False,\n",
        "#     r=32,\n",
        "#     lora_alpha=64,\n",
        "#     lora_dropout=0.1,\n",
        "#     target_modules=target_modules,\n",
        "# )\n",
        "\n",
        "\n",
        "# model = get_peft_model(model, peft_config)\n",
        "# model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "# def count_trainable_parameters(model):\n",
        "# model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "# params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "# return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7abbadfe",
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_trainable_parameters(model):\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "    return params\n",
        "\n",
        "\n",
        "count_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "430aabb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# for experimenting with freezing the core of the model (only the encoder)\n",
        "# for name, param in model.named_parameters():\n",
        "#     if not name.startswith(\"classifier\"):\n",
        "#         param.requires_grad = False\n",
        "#         #print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49dcba23",
      "metadata": {
        "id": "49dcba23"
      },
      "source": [
        "These warnings are telling us that the model is discarding some weights that it used for language modelling (the `lm_head`) and adding some weights for sequence classification (the `classifier`). This is exactly what we expect when we want to fine-tune a language model on a sequence classification task!\n",
        "\n",
        "Next, we initialize our `TrainingArguments`. These control the various training hyperparameters, and will be passed to our `Trainer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "738b8e06",
      "metadata": {},
      "outputs": [],
      "source": [
        "%env WANDB_WATCH=all\n",
        "%env WANDB_SILENT=true\n",
        "%env WANDB_LOG_MODEL=end\n",
        "%env WANDB_PROJECT=medium biosciences\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "version = 1  #experiment version\n",
        "batch_size = 32\n",
        "train_epochs = 100\n",
        "num_workers = 8\n",
        "lr = 1e-5 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "775cb3e7",
      "metadata": {
        "id": "775cb3e7"
      },
      "outputs": [],
      "source": [
        "from transformers import EarlyStoppingCallback\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "early_stopping = EarlyStoppingCallback(early_stopping_patience=5)\n",
        "\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=train_epochs,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    push_to_hub=False,\n",
        "    fp16=True,\n",
        "    fp16_full_eval=True,\n",
        "    # bf16_full_eval=True,\n",
        "    # bf16=True,\n",
        "    save_total_limit=1,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adamw_torch\",\n",
        "    report_to=\"wandb\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.01,\n",
        "    logging_strategy=\"epoch\",\n",
        "    run_name=f\"{model_checkpoint.split('/')[-1]}-v-{version}\",\n",
        "    dataloader_num_workers=num_workers,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc95d099",
      "metadata": {
        "id": "bc95d099"
      },
      "source": [
        "Next, we define the metric we will use to evaluate our models and write a `compute_metrics` function. We can load this from the `evaluate` library. I chose the weighted mode of f1, precision and recall calculation as we have multi-class classification problem with slight difference in the distribution of classes in the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "471cef9f",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "55d5a21f0be54ea1b33c552fed2c3bd3"
          ]
        },
        "id": "471cef9f",
        "outputId": "b096a8bf-35db-4707-8538-d067865e87a4"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.combine([\"f1\", \"precision\", \"recall\"])\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    metrics = metric.compute(\n",
        "        predictions=predictions, references=labels, average=\"weighted\"\n",
        "    )\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e212b751",
      "metadata": {
        "id": "e212b751",
        "outputId": "6bb7723b-2621-4514-ef05-e23e887d4e28"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    # Extract elements\n",
        "    input_ids_sequence = [item[\"input_ids_sequence\"] for item in batch]\n",
        "    attention_mask_sequence = [item[\"attention_mask_sequence\"] for item in batch]\n",
        "    input_ids_structure = [item[\"input_ids_structure\"] for item in batch]\n",
        "    attention_mask_structure = [item[\"attention_mask_structure\"] for item in batch]\n",
        "    labels = [item[\"labels\"] for item in batch]\n",
        "\n",
        "    # Pad sequences to the maximum length in the batch\n",
        "    input_ids_sequence_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        input_ids_sequence, batch_first=True, padding_value=tokenizer.pad_token_id\n",
        "    )\n",
        "    attention_mask_sequence_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        attention_mask_sequence, batch_first=True, padding_value=0\n",
        "    )\n",
        "    input_ids_structure_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        input_ids_structure, batch_first=True, padding_value=tokenizer.pad_token_id\n",
        "    )\n",
        "    attention_mask_structure_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "        attention_mask_structure, batch_first=True, padding_value=0\n",
        "    )\n",
        "\n",
        "    labels = torch.stack(labels)\n",
        "\n",
        "    return {\n",
        "        \"input_ids_sequence\": input_ids_sequence_padded,\n",
        "        \"attention_mask_sequence\": attention_mask_sequence_padded,\n",
        "        \"input_ids_structure\": input_ids_structure_padded,\n",
        "        \"attention_mask_structure\": attention_mask_structure_padded,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[early_stopping],\n",
        "    data_collator=custom_collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c3cf6da",
      "metadata": {
        "id": "9c3cf6da",
        "outputId": "a0780d09-fb66-4f07-b7e6-782fcc85e34b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c67b81d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d701ed",
      "metadata": {
        "id": "78d701ed"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed2481ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7683e199",
      "metadata": {},
      "outputs": [],
      "source": [
        "version = 1\n",
        "batch_size = 64\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import wandb\n",
        "\n",
        "\n",
        "num_labels = max(train_labels + test_labels) + 1\n",
        "from transformers import EarlyStoppingCallback\n",
        "import os\n",
        "\n",
        "\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    push_to_hub=False,\n",
        "    fp16=True,\n",
        "    fp16_full_eval=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.combine([\"f1\", \"precision\", \"recall\"])\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    metrics = metric.compute(\n",
        "        predictions=predictions, references=labels, average=\"weighted\"\n",
        "    )\n",
        "    return metrics\n",
        "\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Use the API to fetch the artifact from wandb\n",
        "# api = wandb.Api()\n",
        "# artifact = api.artifact(\n",
        "#     f\"shahdsaf/medium biosciences/model-{model_checkpoint.split('/')[-1]}-v-{version}:latest\"\n",
        "# )\n",
        "\n",
        "# # Download the artifact to a local directory\n",
        "# model_dir = artifact.download()\n",
        "\n",
        "model_dir = \"\"\n",
        "\n",
        "\n",
        "# Load your Hugging Face model from that folder\n",
        "#  using the same model class\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_dir, num_labels=num_labels\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6508b178",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
