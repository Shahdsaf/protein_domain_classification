{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d62635f5",
   "metadata": {},
   "source": [
    "## This notebook is for running the model at inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d49f39-2b5a-4387-866e-2484f6b7dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "val_df = pd.read_csv(\"./data/val.csv\", index_col=0)\n",
    "\n",
    "test_sequences = val_df[\"sequences\"].tolist()\n",
    "test_sequences_3d = val_df[\"seq3d\"].tolist()\n",
    "test_labels = val_df[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e1113-6a64-401a-a818-ef3c6d06e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "from datasets import Dataset\n",
    "import re\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import T5EncoderModel, T5ForSequenceClassification, T5PreTrainedModel\n",
    "from transformers import PretrainedConfig, T5Config\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.combine([\"f1\", \"precision\", \"recall\"])\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    metrics = metric.compute(\n",
    "        predictions=predictions, references=labels, average=\"weighted\"\n",
    "    )\n",
    "    return metrics\n",
    "\n",
    "\n",
    "class ProteinDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        return {\n",
    "            \"input_ids_sequence\": torch.tensor(item[\"input_ids_sequence\"]),\n",
    "            \"attention_mask_sequence\": torch.tensor(item[\"attention_mask_sequence\"]),\n",
    "            \"input_ids_structure\": torch.tensor(item[\"input_ids_structure\"]),\n",
    "            \"attention_mask_structure\": torch.tensor(item[\"attention_mask_structure\"]),\n",
    "            \"labels\": torch.tensor(item[\"labels\"]),\n",
    "        }\n",
    "\n",
    "\n",
    "def create_dataset(tokenized_sequences, tokenized_structures, labels):\n",
    "    input_ids_sequence = [item[\"input_ids\"].squeeze() for item in tokenized_sequences]\n",
    "    attention_mask_sequence = [\n",
    "        item[\"attention_mask\"].squeeze() for item in tokenized_sequences\n",
    "    ]\n",
    "    input_ids_structure = [item[\"input_ids\"].squeeze() for item in tokenized_structures]\n",
    "    attention_mask_structure = [\n",
    "        item[\"attention_mask\"].squeeze() for item in tokenized_structures\n",
    "    ]\n",
    "\n",
    "    dataset_dict = {\n",
    "        \"input_ids_sequence\": input_ids_sequence,\n",
    "        \"attention_mask_sequence\": attention_mask_sequence,\n",
    "        \"input_ids_structure\": input_ids_structure,\n",
    "        \"attention_mask_structure\": attention_mask_structure,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "    return Dataset.from_dict(dataset_dict)\n",
    "\n",
    "\n",
    "def preprocess_data(sequences, structures, tokenizer):\n",
    "    tokenized_sequences = []\n",
    "    tokenized_structures = []\n",
    "\n",
    "    for sequence, structure in zip(sequences, structures):\n",
    "        # Preprocess sequences\n",
    "        sequence = \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))\n",
    "        structure = \" \".join(list(structure))\n",
    "\n",
    "        sequence = \"<AA2fold> \" + sequence if sequence.isupper() else sequence\n",
    "        structure = \"<fold2AA> \" + structure\n",
    "\n",
    "        # Tokenize sequences and structures\n",
    "        sequence_inputs = tokenizer(\n",
    "            sequence, add_special_tokens=True, padding=\"longest\", return_tensors=\"pt\"\n",
    "        )\n",
    "        structure_inputs = tokenizer(\n",
    "            structure, add_special_tokens=True, padding=\"longest\", return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        tokenized_sequences.append(sequence_inputs)\n",
    "        tokenized_structures.append(structure_inputs)\n",
    "\n",
    "    return tokenized_sequences, tokenized_structures\n",
    "\n",
    "\n",
    "class T5ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config: T5Config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.d_model, config.d_model)\n",
    "        self.dropout = nn.Dropout(p=config.classifier_dropout)\n",
    "        self.out_proj = nn.Linear(config.d_model, config.num_labels)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = torch.tanh(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.out_proj(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class CustomT5ForSequenceClassification(T5PreTrainedModel):\n",
    "    def __init__(self, model_checkpoint, config):\n",
    "\n",
    "        super().__init__(config)\n",
    "        self.transformer = T5EncoderModel.from_pretrained(model_checkpoint)\n",
    "        self.classification_head = T5ClassificationHead(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_sequence,\n",
    "        input_ids_structure,\n",
    "        attention_mask_sequence=None,\n",
    "        attention_mask_structure=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        # here we could add an augmentation step in which we choose x% of the time the sequence only, the structure only, or both\n",
    "\n",
    "        # Get embeddings for the sequence\n",
    "        sequence_outputs = self.transformer(\n",
    "            input_ids_sequence, attention_mask=attention_mask_sequence\n",
    "        )\n",
    "        sequence_embeddings = sequence_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Get embeddings for the structure\n",
    "        structure_outputs = self.transformer(\n",
    "            input_ids_structure, attention_mask=attention_mask_structure\n",
    "        )\n",
    "        structure_embeddings = structure_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Combine the embeddings\n",
    "        combined_embeddings = (\n",
    "            sequence_embeddings + structure_embeddings\n",
    "        ) / 2.0  # can be changed to concatenation but the embedding d_model in the config should be adjusted accordingly\n",
    "\n",
    "        # Feed to classifier head\n",
    "        logits = self.classification_head(combined_embeddings)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "\n",
    "model_checkpoint = \"Rostlab/ProstT5\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "model_dir = (\n",
    "    \"/workspace/cath_classification/artifacts/model-ProstT5-v-11:v1/model.safetensors\"\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_checkpoint, do_lower_case=False)\n",
    "\n",
    "\n",
    "# val_df = pd.read_csv(\"./data/val.csv\")\n",
    "# test_sequences = val_df[\"sequences\"].tolist()\n",
    "# test_sequences_3d = val_df[\"seq3d\"].tolist()\n",
    "# test_labels = val_df[\"label\"].tolist()\n",
    "\n",
    "# Preprocess and tokenize the data\n",
    "test_tokenized_sequences, test_tokenized_structures = preprocess_data(\n",
    "    test_sequences, test_sequences_3d, tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "# Create Dataset objects\n",
    "test_dataset = create_dataset(\n",
    "    test_tokenized_sequences, test_tokenized_structures, test_labels\n",
    ")\n",
    "\n",
    "# Create custom dataset\n",
    "test_dataset = ProteinDataset(test_dataset)\n",
    "\n",
    "num_labels = 10  # max(test_labels) + 1\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "preconfig = PretrainedConfig.from_pretrained(model_checkpoint)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "preconfig.update({\"num_labels\": num_labels, \"classifier_dropout\": 0.1})\n",
    "\n",
    "model = CustomT5ForSequenceClassification(model_checkpoint, preconfig).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70595501-6b1b-470b-b5d6-39cf9fb748a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file as load_safetensors\n",
    "\n",
    "state_dict = load_safetensors(model_dir)\n",
    "\n",
    "# Load the state dict into the model\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be36475b-3380-4a50-add0-961449d98726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Extract elements\n",
    "    input_ids_sequence = [item[\"input_ids_sequence\"] for item in batch]\n",
    "    attention_mask_sequence = [item[\"attention_mask_sequence\"] for item in batch]\n",
    "    input_ids_structure = [item[\"input_ids_structure\"] for item in batch]\n",
    "    attention_mask_structure = [item[\"attention_mask_structure\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    input_ids_sequence_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids_sequence, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    attention_mask_sequence_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        attention_mask_sequence, batch_first=True, padding_value=0\n",
    "    )\n",
    "    input_ids_structure_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids_structure, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    attention_mask_structure_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        attention_mask_structure, batch_first=True, padding_value=0\n",
    "    )\n",
    "\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids_sequence\": input_ids_sequence_padded,\n",
    "        \"attention_mask_sequence\": attention_mask_sequence_padded,\n",
    "        \"input_ids_structure\": input_ids_structure_padded,\n",
    "        \"attention_mask_structure\": attention_mask_structure_padded,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=custom_collate_fn,\n",
    ")\n",
    "trainer.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
